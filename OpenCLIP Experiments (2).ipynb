{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b020e61-a798-4ae2-8f9a-5f13cce2d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import data_utils\n",
    "import similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074be612-2b98-4e62-8376-1577777295f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b44804-6fc0-4b6c-bada-4c7ffa4f45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_name = 'ViT-B/16'\n",
    "target_name = 'resnet50'\n",
    "target_layer = 'layer3'\n",
    "d_probe = 'broden'\n",
    "concept_set = 'data/10k.txt'\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "save_dir = 'saved_activations'\n",
    "similarity_fn = similarity.soft_wpmi\n",
    "\n",
    "\n",
    "save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "\n",
    "target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "similarities, target_feats = utils.get_similarity_from_activations(target_save_name, clip_save_name, \n",
    "                                                             text_save_name, similarity_fn, device=device)\n",
    "\n",
    "with open(concept_set, 'r') as f: \n",
    "    words = (f.read()).split('\\n')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)\n",
    "top_vals, top_ids = torch.topk(target_feats, k=5, dim=0)\n",
    "vals, idsx = torch.topk(similarities,k=1,largest=True)\n",
    "\n",
    "image_sets = [[pil_data[j][0].resize([128,128]) for j in i] for i in np.array(top_ids).T]\n",
    "\n",
    "clip_label = [words[int(i)] for i in idsx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721def3-4ddc-47eb-a09f-67ddb6dfbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba2ace-44e1-4c04-9ea0-2e0fb4f474e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text Corpus\n",
    "with open('data/10k.txt', 'r') as f: \n",
    "    clip_set = (f.read()).split('\\n')\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "model = model.cuda()\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(clip_set).cuda()\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56009f-7e92-4b2f-aac5-51527e7c34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_images_to_tensors(imageset):\n",
    "    tensor_images = [preprocess(image).unsqueeze(0) for image in imageset]\n",
    "    return torch.cat(tensor_images, dim=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525e0ad-67dd-45f4-876b-3380fa021a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff5933-9892-4e5d-87a2-0b8845cb73ac",
   "metadata": {},
   "source": [
    "## Open CLIP Top K results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c164b8f-0fae-4b03-8082-f00835bf6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(imageset):\n",
    "    tensor_images = pil_images_to_tensors(imageset)\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = model.encode_image(tensor_images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    return text_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfe2c8-18bb-4880-9be2-0864ad7e514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_concept(imageset, word_count):\n",
    "    text_probs = get_probs(imageset)\n",
    "\n",
    "    values, indices = torch.topk(text_probs, k=word_count, largest=True)\n",
    "\n",
    "    image_labels_pairs = [\n",
    "        (imageset[i], [words[int(idx)] for idx in indices[i]])\n",
    "        for i in range(len(imageset))\n",
    "    ]\n",
    "\n",
    "    return image_labels_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82413c6-f1f4-47b6-96af-ee819f4c7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk(imageset,label,k):\n",
    "    word_count = int(len(clip_set) * k)\n",
    "    cur_list = []\n",
    "    sum = 0\n",
    "    for pair in pred_concept(imageset,word_count):\n",
    "        if label in pair[1]:\n",
    "            cur_list.append(True)\n",
    "            sum+=1\n",
    "        else:\n",
    "            cur_list.append(False)\n",
    "        \n",
    "    return cur_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320bb01-49bc-4d99-a344-4fa62575d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "multiplier = 0.01\n",
    "for i in range(len(image_sets)):\n",
    "    matches.append(evaluate_topk(image_sets[i],clip_label[i],0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254120e-5928-4ae4-ae6b-6569e6b75634",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655e429-806c-46dd-aa3f-6869f3b602f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches\n",
    "\n",
    "nested_array_np = np.array(matches,dtype=int)\n",
    "# Reshape the array to have Neuron # rows and 5 columns\n",
    "\n",
    "reshaped_array = nested_array_np.reshape(1024, 5)\n",
    "\n",
    "# Create a DataFrame from the reshaped array\n",
    "df = pd.DataFrame(reshaped_array, columns=['Image1', 'Image2', 'Image3', 'Image4', 'Image5'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('openclipcsvs/'+'topk/'+\"_\".join([clip_name,target_name,target_layer,d_probe,concept_set,str(multiplier)]).replace('.','').replace('/','') + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc8656-6fa5-4edc-9e0e-7e022cded150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_labels_topk(image_set, tensor_labels, multiplier):\n",
    "    # Evaluate similarity for the entire set of images\n",
    "    matching = evaluate_topk(image_set, tensor_labels, multiplier)\n",
    "\n",
    "    # Convert threshold_exceed tensor to a boolean array\n",
    "    threshold_exceed_bool = threshold_exceed.cpu().numpy().astype(bool)\n",
    "\n",
    "    # Display the images and labels\n",
    "    print(tensor_labels)\n",
    "    fig, axs = plt.subplots(1, len(image_set), figsize=(15, 5))\n",
    "    for j in range(len(image_set)):\n",
    "        axs[j].imshow(image_set[j])\n",
    "        axs[j].axis('off')\n",
    "        axs[j].set_title(f'Label: {\"Yes\" if threshold_exceed_bool[j] else \"No\"}', color='green' if threshold_exceed_bool[j] else 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(image_sets)):\n",
    "    display_images_with_labels(image_sets[i], clip_label[i], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5950e-765a-478a-9326-ca16633889ec",
   "metadata": {},
   "source": [
    "## Embedding Similarity Score Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c3cc8-6784-416c-96cc-1d466e863859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sim(imageset, word):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        encoded_word = model.encode_text(tokenizer(word).cuda())\n",
    "        encoded_word = encoded_word.float()  # Convert to Float\n",
    "        encoded_word /= encoded_word.norm(dim=-1, keepdim=True)\n",
    "        tensor_images = pil_images_to_tensors(imageset)\n",
    "    \n",
    "        image_features = model.encode_image(tensor_images)\n",
    "        image_features = image_features.float()  # Convert to Float\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate embedding scores using vectorized operations\n",
    "    embedding_scores = torch.matmul(image_features, encoded_word.T)\n",
    "\n",
    "    return embedding_scores\n",
    "\n",
    "def evaluate_sim(imageset, label, threshold):\n",
    "    embedding_scores = embedding_sim(imageset, label)\n",
    "    threshold_exceed = (embedding_scores >= threshold).to(torch.float32)\n",
    "    return threshold_exceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eaf3a7-ac6e-472f-a1cb-b4a9d6a4848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836c164-0fd8-45dc-ade2-813d3709b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.25\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    # Your main processing logic here\n",
    "    sim_matches = []\n",
    "    for i in range(len(image_sets)):\n",
    "        sim_matches.append(evaluate_sim(image_sets[i], clip_label[i], threshold))\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce3c27-9603-474b-b40d-60fcec1eb4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_array_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60932b-4b4d-473f-9b48-3e01f305d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "nested_array_np = np.array([np.array(i.cpu()) for i in sim_matches],dtype=int)\n",
    "# Reshape the array to have Neuron # rows and 5 columns\n",
    "squeezed_array = np.squeeze(nested_array_np, axis=-1)\n",
    "reshaped_array = nested_array_np.reshape(1024, 5)\n",
    "\n",
    "# Create a DataFrame from the reshaped array\n",
    "df = pd.DataFrame(reshaped_array, columns=['Image1', 'Image2', 'Image3', 'Image4', 'Image5'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('openclipcsvs/'+'thresholding/'+\"_\".join([clip_name,target_name,target_layer,d_probe,concept_set,str(threshold)]).replace('.','').replace('/','') + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e3e9e-5cf7-4ab4-a69f-0800c2e9a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming that image_sets is a list of lists containing PIL images\n",
    "# and evaluate_sim returns a list of tensors containing yes or no values\n",
    "\n",
    "def display_images_with_labels(image_set, tensor_labels, threshold):\n",
    "    # Evaluate similarity for the entire set of images\n",
    "    threshold_exceed = evaluate_sim(image_set, tensor_labels, threshold)\n",
    "\n",
    "    # Convert threshold_exceed tensor to a boolean array\n",
    "    threshold_exceed_bool = threshold_exceed.cpu().numpy().astype(bool)\n",
    "\n",
    "    # Display the images and labels\n",
    "    print(tensor_labels)\n",
    "    fig, axs = plt.subplots(1, len(image_set), figsize=(15, 5))\n",
    "    for j in range(len(image_set)):\n",
    "        axs[j].imshow(image_set[j])\n",
    "        axs[j].axis('off')\n",
    "        axs[j].set_title(f'Label: {\"Yes\" if threshold_exceed_bool[j] else \"No\"}', color='green' if threshold_exceed_bool[j] else 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(image_sets)):\n",
    "    display_images_with_labels(image_sets[i], clip_label[i], 0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2283367-8d35-4c4c-80f2-a8b897b69dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(image_sets)):\n",
    "    display_images_with_labels(image_sets[i], tensor_labels[i], threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4ddde-3414-44b9-a6f6-5ad7d7aa1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_label(idx,k):\n",
    "    \n",
    "    obj = pred_concept(image_sets[idx],k)\n",
    "    display(obj[0][0])\n",
    "    display(obj[0][1])\n",
    "    \n",
    "    display(obj[1][0])\n",
    "    display(obj[1][1])\n",
    "    \n",
    "    display(obj[2][0])\n",
    "    display(obj[2][1])\n",
    "    \n",
    "    display(obj[3][0])\n",
    "    display(obj[3][1])\n",
    "\n",
    "    return (obj[0][1],obj[1][1],obj[2][1],obj[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ad808-0be2-4b18-88fa-daaef6f31a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_label(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
